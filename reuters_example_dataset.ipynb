{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from keras import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of top words to load in data\n",
    "num_words = 500\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y data is a single int representing the category. We need to turn this into a vector\n",
    "y_train_vec = np_utils.to_categorical(y_train)\n",
    "y_test_vec = np_utils.to_categorical(y_test)\n",
    "\n",
    "# cutoff or pad data to some max length\n",
    "max_length = 5000 #words\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 5000, 32)          16000     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 160000)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                10240064  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 10,259,054\n",
      "Trainable params: 10,259,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs/reuters{}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 55s 6ms/step - loss: 2.0681 - acc: 0.4961 - val_loss: 1.7297 - val_acc: 0.5904\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 53s 6ms/step - loss: 1.4482 - acc: 0.6527 - val_loss: 1.4815 - val_acc: 0.6536\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 60s 7ms/step - loss: 1.0910 - acc: 0.7400 - val_loss: 1.3555 - val_acc: 0.6852\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 55s 6ms/step - loss: 0.8098 - acc: 0.8094 - val_loss: 1.3484 - val_acc: 0.6910\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 59s 7ms/step - loss: 0.5974 - acc: 0.8652 - val_loss: 1.4114 - val_acc: 0.6839\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 63s 7ms/step - loss: 0.4620 - acc: 0.8981 - val_loss: 1.4464 - val_acc: 0.6941\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 54s 6ms/step - loss: 0.3586 - acc: 0.9270 - val_loss: 1.5217 - val_acc: 0.6883\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 62s 7ms/step - loss: 0.2854 - acc: 0.9405 - val_loss: 1.6383 - val_acc: 0.6870\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 66s 7ms/step - loss: 0.2472 - acc: 0.9485 - val_loss: 1.7262 - val_acc: 0.6781\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 65s 7ms/step - loss: 0.2383 - acc: 0.9520 - val_loss: 1.7429 - val_acc: 0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f57b490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train_vec, \n",
    "          validation_data=(x_test, y_test_vec),\n",
    "          epochs=10, \n",
    "          callbacks=[tensorboard]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
